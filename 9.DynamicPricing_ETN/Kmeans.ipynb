{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6199c06-21ea-4fd1-8ba8-c832352d5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd7d63d-cf49-42af-8bbc-5818908c1be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('PorCorreo.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46604f-78e4-4680-9257-64b0a38813a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3abe7a7-b07f-45b9-9474-43a3e097e867",
   "metadata": {},
   "outputs": [],
   "source": [
    "n=2\n",
    "if n==1:\n",
    "    df=df[df['Primer_Compra']==0]\n",
    "    #df.drop(columns=df.columns[6:-1], axis=1, inplace=True)\n",
    "    df.drop(columns='Primer_Compra', axis=1, inplace=True)\n",
    "elif n==2:\n",
    "    df=df[df['SBol_Vend']>1]\n",
    "elif n==3:\n",
    "    df=df[df['SBol_Vend']==1]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14eb4228-2a33-4db4-aebe-2dc0b62c5b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df[df.columns[1:]]\n",
    "X = df[df.columns[1:]]\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86708f4-4fe8-421f-9730-9a3164404925",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=0\n",
    "if m==0:# genera 7 clusters para una unica compra, y 7 para multiples compras\n",
    "    # 1. Crea una instancia del escalador estándar\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # 2. Aplica el ajuste y la transformación\n",
    "    X_escalado = scaler.fit_transform(X)\n",
    "    figname='2d_pca_clustersKmeansSS.png'\n",
    "elif m==1: # genera dos clusters para una unica compra y 2 para multiples compras\n",
    "    # 1. Crea una instancia del escalador robusto\n",
    "    robust_scaler = RobustScaler()\n",
    "    \n",
    "    # 2. Aplica el ajuste y la transformación\n",
    "    X_escalado = robust_scaler.fit_transform(X)\n",
    "    figname='2d_pca_clustersKmeansRS.png'\n",
    "elif m==2:# genera 9 clusters para una unica compra y 7 para multiples compras\n",
    "    # 1. Crea una instancia del escalador Min-Max\n",
    "    minmax_scaler = MinMaxScaler()\n",
    "    \n",
    "    # 2. Aplica el ajuste y la transformación\n",
    "    X_escalado = minmax_scaler.fit_transform(X)\n",
    "    figname='2d_pca_clustersKmeansMM.png'\n",
    "    \n",
    "# Elegir un valor para min_samples (por ejemplo, 2 veces el número de dimensiones)\n",
    "min_samples_val = 2*len(X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a3e63b-b0fa-4b23-a6d8-6c92db8390d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_escalado = pd.DataFrame(X_escalado, columns=df.columns[1:])\n",
    "# --- Fin del paso de escalado ---\n",
    "\n",
    "# 3. Encontrar la K óptima (usando el Coeficiente de Silueta)\n",
    "max_silhouette_score = -1\n",
    "optimal_k = 0\n",
    "K_range = range(2, 10)\n",
    "\n",
    "for k in K_range:\n",
    "    kmeans_model = KMeans(n_clusters=k, n_init='auto', random_state=42)\n",
    "    kmeans_model.fit(X_escalado)\n",
    "    score = silhouette_score(X_escalado, kmeans_model.labels_)\n",
    "    if score > max_silhouette_score:\n",
    "        max_silhouette_score = score\n",
    "        optimal_k = k\n",
    "print(f\"El número óptimo de clusters (K) es: {optimal_k}\")\n",
    "\n",
    "# 4. Fase de Entrenamiento: Entrenar el modelo final con la K óptima\n",
    "modelo_entrenado = KMeans(n_clusters=optimal_k, n_init='auto', random_state=42)\n",
    "modelo_entrenado.fit(X_escalado)\n",
    "df['Cluster'] = modelo_entrenado.labels_\n",
    "\n",
    "# 5. Analizar el perfil de cada cluster\n",
    "cluster_profile = df.groupby('Cluster')[df.columns[1:]].mean().round(2)\n",
    "\n",
    "print(\"\\n--- Perfil promedio de cada Cluster ---\")\n",
    "print(cluster_profile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3eaaba3-0a80-4dc7-9e80-ddbdd25f9ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Perform PCA to reduce to 2 dimensions\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_escalado) # Assuming X_escalado is your scaled data\n",
    "\n",
    "# Create a new DataFrame with the PCA results and cluster labels\n",
    "pca_df = pd.DataFrame(data = X_pca, columns = ['principal component 1', 'principal component 2'])\n",
    "pca_df['Cluster'] = df['Cluster'].values\n",
    "\n",
    "# Visualize the clusters in 2D\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='principal component 1', y='principal component 2', hue='Cluster', data=pca_df, palette=\"Accent\", legend='full')\n",
    "plt.title('2D PCA of Clusters')\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.grid(True)\n",
    "# Save the plot as a PNG file\n",
    "plt.savefig(figname)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d60953-fa37-4648-803b-4931ddce748a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Asumimos que X_escalado ya existe y tiene nombres de columnas (si no, usa X.columns)\n",
    "# Ejemplo: si X_escalado es un array de numpy, necesitamos los nombres de las características originales.\n",
    "# Si X es un DataFrame de Pandas:\n",
    "# nombres_originales = X.columns\n",
    "# Si X es un array de numpy con 5 columnas:\n",
    "# nombres_originales = [f'feature_{i+1}' for i in range(X_escalado.shape[1])]\n",
    "\n",
    "# IMPORTANTE: Asegúrate de reemplazar 'X.columns' si tus datos originales no son un DataFrame\n",
    "try:\n",
    "    nombres_originales = X.columns  # Intenta obtener nombres de un DataFrame\n",
    "except AttributeError:\n",
    "    # Si X es un array de numpy, usa la forma de abajo para obtener el número de columnas\n",
    "    nombres_originales = [f'Variable_{i+1}' for i in range(X_escalado.shape[1])]\n",
    "\n",
    "\n",
    "# 1. Ejecutar PCA con TODAS las componentes posibles\n",
    "pca = PCA(n_components=None)\n",
    "pca.fit(X_escalado)\n",
    "\n",
    "# --- 1. RESUMEN DE VARIANZA (Igual que antes) ---\n",
    "\n",
    "# Extraer la información clave\n",
    "varianza_individual = pca.explained_variance_ratio_\n",
    "varianza_acumulada = np.cumsum(varianza_individual)\n",
    "nombres_componentes = [f'CP{i+1}' for i in range(len(varianza_individual))]\n",
    "\n",
    "# Crear el DataFrame de resumen de varianza\n",
    "df_pca_resumen = pd.DataFrame({\n",
    "    'Componente Principal': nombres_componentes,\n",
    "    'Varianza Explicada (%)': varianza_individual * 100,\n",
    "    'Varianza Acumulada (%)': varianza_acumulada * 100\n",
    "})\n",
    "\n",
    "print(\"## 1. Resumen de la Varianza Explicada por Componente\")\n",
    "# Usamos to_string() para asegurar la impresión en consola\n",
    "print(df_pca_resumen.head(10).style.format({\n",
    "    'Varianza Explicada (%)': \"{:.2f}%\",\n",
    "    'Varianza Acumulada (%)': \"{:.2f}%\"\n",
    "}).to_string())\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# --- 2. CARGAS (LOADINGS) DE LAS COMPONENTES ---\n",
    "\n",
    "# La matriz de cargas (loadings) se encuentra en pca.components_\n",
    "# Esto nos dice cómo se relaciona cada variable original con cada CP.\n",
    "cargas_pca = pca.components_\n",
    "\n",
    "# 4. Crear el DataFrame de cargas\n",
    "# Las filas son las Componentes Principales (CP1, CP2, etc.)\n",
    "# Las columnas son las Variables Originales\n",
    "df_cargas = pd.DataFrame(\n",
    "    cargas_pca.T,\n",
    "    columns=nombres_componentes,\n",
    "    index=nombres_originales\n",
    ")\n",
    "\n",
    "# 5. Mostrar la tabla de cargas (usamos las 3-5 primeras componentes para interpretación)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"## 2. Contribución de Variables a las Componentes Principales (Loadings)\")\n",
    "print(\"Los valores cercanos a 1 o -1 indican una fuerte contribución.\")\n",
    "\n",
    "# Mostraremos solo las primeras 5 componentes para facilitar la lectura\n",
    "df_cargas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f2c99c-4242-4f6b-8d3f-53767a3699b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import plotly.express as px\n",
    "\n",
    "# Perform PCA to reduce to 3 dimensions\n",
    "pca_3d = PCA(n_components=3)\n",
    "X_pca_3d = pca_3d.fit_transform(X_escalado) # Assuming X_escalado is your scaled data\n",
    "\n",
    "# Create a new DataFrame with the 3D PCA results and cluster labels\n",
    "pca_df_3d = pd.DataFrame(data = X_pca_3d, columns = ['principal component 1', 'principal component 2', 'principal component 3'])\n",
    "pca_df_3d['Cluster'] = df['Cluster'] # Assuming df has the 'Cluster' column\n",
    "\n",
    "# Visualize the clusters in 3D using Plotly\n",
    "fig = px.scatter_3d(pca_df_3d,\n",
    "                    x='principal component 1',\n",
    "                    y='principal component 2',\n",
    "                    z='principal component 3',\n",
    "                    color='Cluster',\n",
    "                    title='3D PCA of Clusters')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bf8097-a012-4314-becd-37461a05c1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "num=0;\n",
    "if num==1:\n",
    "    cluster_profile.to_excel('ResumenMayorA1.xlsx')\n",
    "    df.to_excel('Clus4VentasMayorA1.xlsx')\n",
    "elif num==2:\n",
    "    cluster_profile.to_excel('ResumenMenorA1.xlsx')\n",
    "    df.to_excel('Clus4VentasMenorA1.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22a6d98-5d2c-41f0-a8c3-79f9d070e6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f539ff0e-d521-49f6-a237-fa41ef442e4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
